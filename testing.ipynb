{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicting with bart large trained on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_parquet('test_data.parquet')\n",
    "from datasets import Dataset\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrads/shahr072/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Specify the path to your saved model directory\n",
    "model_path = \"./saved_model\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./trained_tokenizer_bart_large_on_full_dataset\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./trained_model_bart_large_on_full_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11334/11334 [00:07<00:00, 1541.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_data(batch):\n",
    "    inputs = tokenizer(batch['document'], max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    outputs = tokenizer(batch['summary'], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    inputs['labels'] = outputs['input_ids']\n",
    "    return inputs\n",
    "\n",
    "# Preprocess test data using Hugging Face Dataset map function\n",
    "tokenized_test = test_dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Document</th>\n",
       "      <th>Reference Summary</th>\n",
       "      <th>Generated Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prison Link Cymru had 1,099 referrals in 2015-...</td>\n",
       "      <td>There is a \"chronic\" need for more housing for...</td>\n",
       "      <td>More affordable homes should be built for ex-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Officers searched properties in the Waterfront...</td>\n",
       "      <td>A man has appeared in court after firearms, am...</td>\n",
       "      <td>A man has been charged in connection with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jordan Hill, Brittany Covington and Tesfaye Co...</td>\n",
       "      <td>Four people accused of kidnapping and torturin...</td>\n",
       "      <td>A judge has ordered four people accused of bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 48-year-old former Arsenal goalkeeper play...</td>\n",
       "      <td>West Brom have appointed Nicky Hammond as tech...</td>\n",
       "      <td>West Brom have appointed Steve Round as their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Restoring the function of the organ - which he...</td>\n",
       "      <td>The pancreas can be triggered to regenerate it...</td>\n",
       "      <td>A short, intense fasting regime can restore th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>But there certainly should be.\\nThese are two ...</td>\n",
       "      <td>Since their impending merger was announced in ...</td>\n",
       "      <td>The merger of the world's two biggest eyewear ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Media playback is not supported on this device...</td>\n",
       "      <td>A \"medal at any cost\" approach created a \"cult...</td>\n",
       "      <td>Wendy Houvenaghel says she was dropped from Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It's no joke. But Kareem Badr says people did ...</td>\n",
       "      <td>Have you heard the one about the computer prog...</td>\n",
       "      <td>If you go to a comedy club in the US and there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Relieved that the giant telecoms company would...</td>\n",
       "      <td>The reaction from BT's investors told us much ...</td>\n",
       "      <td>When news of the proposed merger of BT and Ofc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"I'm really looking forward to it - the home o...</td>\n",
       "      <td>Manager Brendan Rodgers is sure Celtic can exp...</td>\n",
       "      <td>Celtic manager Brendan Rodgers is relishing th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Actual Document  \\\n",
       "0  Prison Link Cymru had 1,099 referrals in 2015-...   \n",
       "1  Officers searched properties in the Waterfront...   \n",
       "2  Jordan Hill, Brittany Covington and Tesfaye Co...   \n",
       "3  The 48-year-old former Arsenal goalkeeper play...   \n",
       "4  Restoring the function of the organ - which he...   \n",
       "5  But there certainly should be.\\nThese are two ...   \n",
       "6  Media playback is not supported on this device...   \n",
       "7  It's no joke. But Kareem Badr says people did ...   \n",
       "8  Relieved that the giant telecoms company would...   \n",
       "9  \"I'm really looking forward to it - the home o...   \n",
       "\n",
       "                                   Reference Summary  \\\n",
       "0  There is a \"chronic\" need for more housing for...   \n",
       "1  A man has appeared in court after firearms, am...   \n",
       "2  Four people accused of kidnapping and torturin...   \n",
       "3  West Brom have appointed Nicky Hammond as tech...   \n",
       "4  The pancreas can be triggered to regenerate it...   \n",
       "5  Since their impending merger was announced in ...   \n",
       "6  A \"medal at any cost\" approach created a \"cult...   \n",
       "7  Have you heard the one about the computer prog...   \n",
       "8  The reaction from BT's investors told us much ...   \n",
       "9  Manager Brendan Rodgers is sure Celtic can exp...   \n",
       "\n",
       "                                   Generated Summary  \n",
       "0  More affordable homes should be built for ex-p...  \n",
       "1  A man has been charged in connection with the ...  \n",
       "2  A judge has ordered four people accused of bea...  \n",
       "3  West Brom have appointed Steve Round as their ...  \n",
       "4  A short, intense fasting regime can restore th...  \n",
       "5  The merger of the world's two biggest eyewear ...  \n",
       "6  Wendy Houvenaghel says she was dropped from Br...  \n",
       "7  If you go to a comedy club in the US and there...  \n",
       "8  When news of the proposed merger of BT and Ofc...  \n",
       "9  Celtic manager Brendan Rodgers is relishing th...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict summaries for the first 10 documents\n",
    "predictions = []\n",
    "documents = tokenized_test[\"document\"][:10]  # Replace \"document\" with your dataset's input column name\n",
    "\n",
    "for doc in documents:\n",
    "    # Tokenize the input document\n",
    "    inputs = tokenizer(doc, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, min_length=30, length_penalty=2.0, num_beams=4)\n",
    "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Store the generated summary\n",
    "    predictions.append(generated_summary)\n",
    "\n",
    "\n",
    "# Retrieve actual documents and reference summaries\n",
    "actual_documents = [tokenized_test[i][\"document\"] for i in range(10)]  # Replace \"document\" with your actual column name\n",
    "reference_summaries = [tokenized_test[i][\"summary\"] for i in range(10)]  # Replace \"summary\" with your actual column name\n",
    "\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual Document\": actual_documents,\n",
    "    \"Reference Summary\": reference_summaries,\n",
    "    \"Generated Summary\": predictions\n",
    "})\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "display(results_df)\n",
    "\n",
    "\n",
    "results_df.to_csv(\"summary_predictions_with_bart_large_on_test_set.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicting with t5-small on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_parquet('test_data.parquet')\n",
    "from datasets import Dataset\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11334/11334 [00:08<00:00, 1305.49 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Document</th>\n",
       "      <th>Reference Summary</th>\n",
       "      <th>Generated Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prison Link Cymru had 1,099 referrals in 2015-...</td>\n",
       "      <td>There is a \"chronic\" need for more housing for...</td>\n",
       "      <td>The need for housing for prison leavers in Wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Officers searched properties in the Waterfront...</td>\n",
       "      <td>A man has appeared in court after firearms, am...</td>\n",
       "      <td>A man has appeared in court charged with firea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jordan Hill, Brittany Covington and Tesfaye Co...</td>\n",
       "      <td>Four people accused of kidnapping and torturin...</td>\n",
       "      <td>Four men have appeared in court charged with a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 48-year-old former Arsenal goalkeeper play...</td>\n",
       "      <td>West Brom have appointed Nicky Hammond as tech...</td>\n",
       "      <td>West Brom have appointed West Brom's former yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Restoring the function of the organ - which he...</td>\n",
       "      <td>The pancreas can be triggered to regenerate it...</td>\n",
       "      <td>A diet that regenerated a special type of cell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>But there certainly should be.\\nThese are two ...</td>\n",
       "      <td>Since their impending merger was announced in ...</td>\n",
       "      <td>The UK eyewear industry has merged with the UK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Media playback is not supported on this device...</td>\n",
       "      <td>A \"medal at any cost\" approach created a \"cult...</td>\n",
       "      <td>British Cycling's new chair Jonathan Browning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It's no joke. But Kareem Badr says people did ...</td>\n",
       "      <td>Have you heard the one about the computer prog...</td>\n",
       "      <td>US comedian Kareem Badr says he was able to ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Relieved that the giant telecoms company would...</td>\n",
       "      <td>The reaction from BT's investors told us much ...</td>\n",
       "      <td>Ofcom's chief executive has said a break-up of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"I'm really looking forward to it - the home o...</td>\n",
       "      <td>Manager Brendan Rodgers is sure Celtic can exp...</td>\n",
       "      <td>Celtic midfielder David Rodgers says he is \"re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Actual Document  \\\n",
       "0  Prison Link Cymru had 1,099 referrals in 2015-...   \n",
       "1  Officers searched properties in the Waterfront...   \n",
       "2  Jordan Hill, Brittany Covington and Tesfaye Co...   \n",
       "3  The 48-year-old former Arsenal goalkeeper play...   \n",
       "4  Restoring the function of the organ - which he...   \n",
       "5  But there certainly should be.\\nThese are two ...   \n",
       "6  Media playback is not supported on this device...   \n",
       "7  It's no joke. But Kareem Badr says people did ...   \n",
       "8  Relieved that the giant telecoms company would...   \n",
       "9  \"I'm really looking forward to it - the home o...   \n",
       "\n",
       "                                   Reference Summary  \\\n",
       "0  There is a \"chronic\" need for more housing for...   \n",
       "1  A man has appeared in court after firearms, am...   \n",
       "2  Four people accused of kidnapping and torturin...   \n",
       "3  West Brom have appointed Nicky Hammond as tech...   \n",
       "4  The pancreas can be triggered to regenerate it...   \n",
       "5  Since their impending merger was announced in ...   \n",
       "6  A \"medal at any cost\" approach created a \"cult...   \n",
       "7  Have you heard the one about the computer prog...   \n",
       "8  The reaction from BT's investors told us much ...   \n",
       "9  Manager Brendan Rodgers is sure Celtic can exp...   \n",
       "\n",
       "                                   Generated Summary  \n",
       "0  The need for housing for prison leavers in Wal...  \n",
       "1  A man has appeared in court charged with firea...  \n",
       "2  Four men have appeared in court charged with a...  \n",
       "3  West Brom have appointed West Brom's former yo...  \n",
       "4  A diet that regenerated a special type of cell...  \n",
       "5  The UK eyewear industry has merged with the UK...  \n",
       "6  British Cycling's new chair Jonathan Browning ...  \n",
       "7  US comedian Kareem Badr says he was able to ta...  \n",
       "8  Ofcom's chief executive has said a break-up of...  \n",
       "9  Celtic midfielder David Rodgers says he is \"re...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./trained_tokenizer_t5-small\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./trained_model_t5-small\")\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_data(batch):\n",
    "    inputs = tokenizer(batch['document'], max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    outputs = tokenizer(batch['summary'], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    inputs['labels'] = outputs['input_ids']\n",
    "    return inputs\n",
    "\n",
    "# Preprocess test data using Hugging Face Dataset map function\n",
    "tokenized_test = test_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# Generate predictions for the first 10 rows\n",
    "predictions = []\n",
    "for i in range(10):\n",
    "    # Get the input document\n",
    "    input_text = tokenized_test[i][\"document\"]  # Replace \"document\" with your actual column name\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=55, min_length=5, length_penalty=2.0, num_beams=4)\n",
    "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Save the generated summary\n",
    "    predictions.append(generated_summary)\n",
    "\n",
    "# Retrieve actual documents and reference summaries\n",
    "actual_documents = [tokenized_test[i][\"document\"] for i in range(10)]  # Replace \"document\" with your actual column name\n",
    "reference_summaries = [tokenized_test[i][\"summary\"] for i in range(10)]  # Replace \"summary\" with your actual column name\n",
    "\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual Document\": actual_documents,\n",
    "    \"Reference Summary\": reference_summaries,\n",
    "    \"Generated Summary\": predictions\n",
    "})\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "display(results_df)\n",
    "\n",
    "\n",
    "results_df.to_csv(\"summary_predictions_with_t5-small_on_test_set.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
