{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_results\t test_data.parquet   trained_model\n",
      "nlp_project.ipynb\t test.ipynb\t     trained_model_t5-small\n",
      "results\t\t\t test.py\t     trained_tokenizer_t5-small\n",
      "summary_predictions.csv  train_data.parquet  val_data.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parquet files from drive\n",
    "train_data = pd.read_parquet('train_data.parquet')\n",
    "val_data = pd.read_parquet('val_data.parquet')\n",
    "test_data = pd.read_parquet('test_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available\n",
    "print(torch.cuda.device_count())  # Number of GPUs available\n",
    "print(torch.cuda.get_device_name(0))  # Name of the first GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrads/shahr072/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert Pandas DataFrame to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and select the same random rows each time\n",
    "small_train_dataset = train_dataset.shuffle(seed=42).select(range(20000))\n",
    "small_val_dataset = val_dataset.shuffle(seed=42).select(range(4000))\n",
    "small_test_dataset = test_dataset.shuffle(seed=42).select(range(4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-xsum\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:38<00:00, 521.96 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:08<00:00, 488.00 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:08<00:00, 487.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_data(batch):\n",
    "    inputs = tokenizer(batch['document'], max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    outputs = tokenizer(batch['summary'], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    inputs['labels'] = outputs['input_ids']\n",
    "    return inputs\n",
    "\n",
    "# Preprocess training data using Hugging Face Dataset map function\n",
    "tokenized_train = small_train_dataset.map(preprocess_data, batched=True)\n",
    "tokenized_val = small_val_dataset.map(preprocess_data, batched=True)\n",
    "tokenized_test = small_test_dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model.device)\n",
    "model = model.to('cuda')\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrads/shahr072/.local/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3768519/2180880813.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/home/csgrads/shahr072/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [936/936 59:09, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.374203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.448444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrads/shahr072/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/home/csgrads/shahr072/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=936, training_loss=0.24481615245851696, metrics={'train_runtime': 3557.4509, 'train_samples_per_second': 16.866, 'train_steps_per_second': 0.263, 'total_flos': 1.2981823408491725e+17, 'train_loss': 0.24481615245851696, 'epoch': 2.9952})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", # Specifies the directory where model checkpoints, logs, and outputs will be saved during training. Useful for resuming training later or for deployment.\n",
    "    evaluation_strategy=\"epoch\", #Indicates when evaluation should be performed. epoch: Evaluates at the end of every training epoch. steps: Evaluates every eval_steps (e.g., every 500 steps). no: Skips evaluation\n",
    "    learning_rate=5e-5, # Sets the learning rate for the optimizer. 5e-5 is a common default for fine-tuning transformer models.\n",
    "    per_device_train_batch_size=16, # The batch size for training on each device (e.g., per GPU or TPU core). If using 2 GPUs, the effective batch size becomes 2 x num_gpus.\n",
    "    per_device_eval_batch_size=16, # The batch size for evaluation, handled similarly to training batch size.\n",
    "    num_train_epochs=3, # The number of complete passes (epochs) through the entire training dataset.\n",
    "    save_steps=20000, # Saves a checkpoint of the model every 10,000 steps. This is useful for resuming training after interruptions.\n",
    "    save_total_limit=2, # Limits the number of saved checkpoints. The oldest checkpoints are deleted once the limit is reached.\n",
    "    fp16=True,  # Enable mixed precision for faster training\n",
    "    remove_unused_columns=True,\n",
    "    gradient_accumulation_steps=2,  # Effective batch size = per_device_train_batch_size * gradient_accumulation_steps\n",
    "    dataloader_num_workers=32,  # Adjust based on available CPU cores\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_tokenizer_bart_large/tokenizer_config.json',\n",
       " './trained_tokenizer_bart_large/special_tokens_map.json',\n",
       " './trained_tokenizer_bart_large/vocab.json',\n",
       " './trained_tokenizer_bart_large/merges.txt',\n",
       " './trained_tokenizer_bart_large/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  \n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./trained_model_bart_large\")\n",
    "tokenizer.save_pretrained(\"./trained_tokenizer_bart_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.4207632431386984, 'rouge2': 0.19567476657665828, 'rougeL': 0.3402484112193753, 'rougeLsum': 0.3399655224916929}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "metric = load(\"rouge\")\n",
    "\n",
    "def evaluate_summaries(model, tokenizer, data):\n",
    "    summaries = []\n",
    "    for sample in data:\n",
    "        inputs = tokenizer(sample['document'], return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "\n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        output = model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
    "        summaries.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "    return summaries\n",
    "\n",
    "# Get predictions\n",
    "test_summaries = evaluate_summaries(model, tokenizer, small_test_dataset)\n",
    "\n",
    "# Compute ROUGE\n",
    "results = metric.compute(predictions=test_summaries, references=small_test_dataset['summary'])\n",
    "print(\"ROUGE Scores:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Specify the path to your saved model directory\n",
    "model_path = \"./saved_model\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./trained_tokenizer_bart_large\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./trained_model_bart_large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Document</th>\n",
       "      <th>Reference Summary</th>\n",
       "      <th>Generated Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patrick Joseph Connors, 59, his son Patrick De...</td>\n",
       "      <td>Three family members have been jailed for forc...</td>\n",
       "      <td>A woman who died after a minibus was hit by a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The visitors were closing in on three points t...</td>\n",
       "      <td>Championship leaders Hibernian twice came from...</td>\n",
       "      <td>A new manor in Dunham Massey has been transfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kamiyah Mobley, who was abducted in July 1998,...</td>\n",
       "      <td>A girl stolen as a newborn from a hospital in ...</td>\n",
       "      <td>Walt Disney World has installed a lighthouse t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Global Slavery Index 2013 says India has t...</td>\n",
       "      <td>Nearly 30 million people around the world are ...</td>\n",
       "      <td>Northern Ireland is hoping to sell beef and ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The duo impressed against New Zealand last wee...</td>\n",
       "      <td>Northern Ireland boss Michael O'Neill looks se...</td>\n",
       "      <td>Blackpool have signed Blackpool midfielder Dav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Seven of the group at Pembroke Comprehensive S...</td>\n",
       "      <td>A group of pupils have been treated at a Pembr...</td>\n",
       "      <td>Scotland head coach Mark Strachan says he will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ms Deacon will succeed Ian McKay when he stand...</td>\n",
       "      <td>Former Scottish health minister Susan Deacon i...</td>\n",
       "      <td>A man has been jailed for four-and-a-half year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>El Nacional in the Dominican Republic has now ...</td>\n",
       "      <td>Actor Alec Baldwin's impression on Saturday Ni...</td>\n",
       "      <td>A man has been charged with attempted murder a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Official documents obtained by Reuters news ag...</td>\n",
       "      <td>The US government is concerned it could be imp...</td>\n",
       "      <td>Wakefield Dragons midfielder Aiton has been di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16 May 2016 Last updated at 08:42 BST\\nMerafie...</td>\n",
       "      <td>Footage has been released showing the demoliti...</td>\n",
       "      <td>Coventry City Council has met with the club's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Actual Document  \\\n",
       "0  Patrick Joseph Connors, 59, his son Patrick De...   \n",
       "1  The visitors were closing in on three points t...   \n",
       "2  Kamiyah Mobley, who was abducted in July 1998,...   \n",
       "3  The Global Slavery Index 2013 says India has t...   \n",
       "4  The duo impressed against New Zealand last wee...   \n",
       "5  Seven of the group at Pembroke Comprehensive S...   \n",
       "6  Ms Deacon will succeed Ian McKay when he stand...   \n",
       "7  El Nacional in the Dominican Republic has now ...   \n",
       "8  Official documents obtained by Reuters news ag...   \n",
       "9  16 May 2016 Last updated at 08:42 BST\\nMerafie...   \n",
       "\n",
       "                                   Reference Summary  \\\n",
       "0  Three family members have been jailed for forc...   \n",
       "1  Championship leaders Hibernian twice came from...   \n",
       "2  A girl stolen as a newborn from a hospital in ...   \n",
       "3  Nearly 30 million people around the world are ...   \n",
       "4  Northern Ireland boss Michael O'Neill looks se...   \n",
       "5  A group of pupils have been treated at a Pembr...   \n",
       "6  Former Scottish health minister Susan Deacon i...   \n",
       "7  Actor Alec Baldwin's impression on Saturday Ni...   \n",
       "8  The US government is concerned it could be imp...   \n",
       "9  Footage has been released showing the demoliti...   \n",
       "\n",
       "                                   Generated Summary  \n",
       "0  A woman who died after a minibus was hit by a ...  \n",
       "1  A new manor in Dunham Massey has been transfor...  \n",
       "2  Walt Disney World has installed a lighthouse t...  \n",
       "3  Northern Ireland is hoping to sell beef and ch...  \n",
       "4  Blackpool have signed Blackpool midfielder Dav...  \n",
       "5  Scotland head coach Mark Strachan says he will...  \n",
       "6  A man has been jailed for four-and-a-half year...  \n",
       "7  A man has been charged with attempted murder a...  \n",
       "8  Wakefield Dragons midfielder Aiton has been di...  \n",
       "9  Coventry City Council has met with the club's ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./trained_tokenizer_t5-small\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./trained_model_t5-small\")\n",
    "\n",
    "# Generate predictions for the first 10 rows\n",
    "predictions = []\n",
    "for i in range(10):\n",
    "    # Get the input document\n",
    "    input_text = tokenized_test[i][\"document\"]  # Replace \"document\" with your actual column name\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=55, min_length=5, length_penalty=2.0, num_beams=4)\n",
    "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Save the generated summary\n",
    "    predictions.append(generated_summary)\n",
    "\n",
    "# Retrieve actual documents and reference summaries\n",
    "actual_documents = [tokenized_val[i][\"document\"] for i in range(10)]  # Replace \"document\" with your actual column name\n",
    "reference_summaries = [tokenized_val[i][\"summary\"] for i in range(10)]  # Replace \"summary\" with your actual column name\n",
    "\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual Document\": actual_documents,\n",
    "    \"Reference Summary\": reference_summaries,\n",
    "    \"Generated Summary\": predictions\n",
    "})\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "display(results_df)\n",
    "\n",
    "\n",
    "results_df.to_csv(\"summary_predictions_with_t5-small.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Document</th>\n",
       "      <th>Reference Summary</th>\n",
       "      <th>Generated Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patrick Joseph Connors, 59, his son Patrick De...</td>\n",
       "      <td>Three family members have been jailed for forc...</td>\n",
       "      <td>A woman who was on her way to her hen party wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The visitors were closing in on three points t...</td>\n",
       "      <td>Championship leaders Hibernian twice came from...</td>\n",
       "      <td>A 1,000-year-old hall has reopened to the publ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kamiyah Mobley, who was abducted in July 1998,...</td>\n",
       "      <td>A girl stolen as a newborn from a hospital in ...</td>\n",
       "      <td>A lighthouse has been erected at Walt Disney W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Global Slavery Index 2013 says India has t...</td>\n",
       "      <td>Nearly 30 million people around the world are ...</td>\n",
       "      <td>The agriculture minister has said she hopes to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The duo impressed against New Zealand last wee...</td>\n",
       "      <td>Northern Ireland boss Michael O'Neill looks se...</td>\n",
       "      <td>Blackpool have signed Leicester City defender ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Seven of the group at Pembroke Comprehensive S...</td>\n",
       "      <td>A group of pupils have been treated at a Pembr...</td>\n",
       "      <td>Gordon Strachan insists there is \"no getting a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ms Deacon will succeed Ian McKay when he stand...</td>\n",
       "      <td>Former Scottish health minister Susan Deacon i...</td>\n",
       "      <td>A man jailed for filming a sex offence in Dund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>El Nacional in the Dominican Republic has now ...</td>\n",
       "      <td>Actor Alec Baldwin's impression on Saturday Ni...</td>\n",
       "      <td>A man from Ireland has appeared in court in co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Official documents obtained by Reuters news ag...</td>\n",
       "      <td>The US government is concerned it could be imp...</td>\n",
       "      <td>Newport Gwent Dragons half-back Brett Ferres h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16 May 2016 Last updated at 08:42 BST\\nMerafie...</td>\n",
       "      <td>Footage has been released showing the demoliti...</td>\n",
       "      <td>Coventry City Football Club has asked the city...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Actual Document  \\\n",
       "0  Patrick Joseph Connors, 59, his son Patrick De...   \n",
       "1  The visitors were closing in on three points t...   \n",
       "2  Kamiyah Mobley, who was abducted in July 1998,...   \n",
       "3  The Global Slavery Index 2013 says India has t...   \n",
       "4  The duo impressed against New Zealand last wee...   \n",
       "5  Seven of the group at Pembroke Comprehensive S...   \n",
       "6  Ms Deacon will succeed Ian McKay when he stand...   \n",
       "7  El Nacional in the Dominican Republic has now ...   \n",
       "8  Official documents obtained by Reuters news ag...   \n",
       "9  16 May 2016 Last updated at 08:42 BST\\nMerafie...   \n",
       "\n",
       "                                   Reference Summary  \\\n",
       "0  Three family members have been jailed for forc...   \n",
       "1  Championship leaders Hibernian twice came from...   \n",
       "2  A girl stolen as a newborn from a hospital in ...   \n",
       "3  Nearly 30 million people around the world are ...   \n",
       "4  Northern Ireland boss Michael O'Neill looks se...   \n",
       "5  A group of pupils have been treated at a Pembr...   \n",
       "6  Former Scottish health minister Susan Deacon i...   \n",
       "7  Actor Alec Baldwin's impression on Saturday Ni...   \n",
       "8  The US government is concerned it could be imp...   \n",
       "9  Footage has been released showing the demoliti...   \n",
       "\n",
       "                                   Generated Summary  \n",
       "0  A woman who was on her way to her hen party wh...  \n",
       "1  A 1,000-year-old hall has reopened to the publ...  \n",
       "2  A lighthouse has been erected at Walt Disney W...  \n",
       "3  The agriculture minister has said she hopes to...  \n",
       "4  Blackpool have signed Leicester City defender ...  \n",
       "5  Gordon Strachan insists there is \"no getting a...  \n",
       "6  A man jailed for filming a sex offence in Dund...  \n",
       "7  A man from Ireland has appeared in court in co...  \n",
       "8  Newport Gwent Dragons half-back Brett Ferres h...  \n",
       "9  Coventry City Football Club has asked the city...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict summaries for the first 10 documents\n",
    "predictions = []\n",
    "documents = tokenized_test[\"document\"][:10]  # Replace \"document\" with your dataset's input column name\n",
    "\n",
    "for doc in documents:\n",
    "    # Tokenize the input document\n",
    "    inputs = tokenizer(doc, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, min_length=30, length_penalty=2.0, num_beams=4)\n",
    "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Store the generated summary\n",
    "    predictions.append(generated_summary)\n",
    "\n",
    "\n",
    "# Retrieve actual documents and reference summaries\n",
    "actual_documents = [tokenized_val[i][\"document\"] for i in range(10)]  # Replace \"document\" with your actual column name\n",
    "reference_summaries = [tokenized_val[i][\"summary\"] for i in range(10)]  # Replace \"summary\" with your actual column name\n",
    "\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual Document\": actual_documents,\n",
    "    \"Reference Summary\": reference_summaries,\n",
    "    \"Generated Summary\": predictions\n",
    "})\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "display(results_df)\n",
    "\n",
    "\n",
    "results_df.to_csv(\"summary_predictions_with_bart_large.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
